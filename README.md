
# Курс по распределенным системам, читаемый в 5 семестре

## Условия Задач 

### MPI/OPENMP

#### Задача № 0 (Сумма элементов массива)
Необходимо создать массив a размером N. 
Предполагается, что запуск исполняемого файла будет происходить с использованием p процессов.
Один из p процессов («основной») заполняет элементы массива следующим образом:
a[i] = i, где 0 <= i < N – индекс массива.
После заполнения массива этот же «основной» процесс по вашему выбору:
1) условно разбивает массив a[N] на p частей
и элементы каждой из (p - 1) частей рассылает остальным (p - 1) процессам 
(с одной из частей массива работает «основной» процесс);

1) условно разбивает массив a на (p - 1) частей
и элементы каждой из частей рассылает остальным (p - 1) процессам
(«основной» процесс занимается только распределением и сборкой информации).
Далее каждый из процессов, получивших свой объем элементов должен найти сумму этих элементов
и отправить ее «основному» процессу.
«Основной» процесс получает все суммы от процессов-рабочих и,
складывая их, получает сумму S всех элементов массива a[N]. 

**Задание:**

1) Вывести на экран в столбик суммы, посчитанные каждым из процессов-рабочих с указанием его номера.
1) Вывести на экран сумму S всех элементов массива, посчитанную сложением всех сумм,
полученных «основным» процессом от процессов-рабочих.
1) Вывести на экран сумму S<sub>0</sub> всех элементов массива,
посчитанную «основным» процессом последовательно. Сравнить ее с суммой S.



#### Задача №1 (Решение одномерного однородного уравнения теплопроводности)

**Постановка задачи:**

Стержень длиной _l_ = 1 в момент времени _t_<sub>0</sub> = 0 имеет температуру _u_<sub>0</sub> = 1.
Температура окружающей среды поддерживается равной 0.
Начальное условие: _u_(_x_, 0) = _u_<sub>0</sub>.
Граничное условие: _u_(0, _t_) = _u_(_l_, _t_) = 0.
Необходимо решить одномерное однородное уравнение теплопроводности.

**Задание:**
1) Получить распределение температуры вдоль стержня
на момент времени _Т_ = 0,1, используя следующие параметры:
_k_ = 1, _h_ = 0,02; _dt_ = 0,0002 _(см. примечание 1)_.
Вывести на экран значения температуры в 11-ти (включая краевые) точках,
т.е. на концах малых отрезков длиной 0,1.
1) Сравнить с точным решением (решаемым в этой же программе):
![](img/01-solution.png)
1) Построить на одной координатной плоскости 3 графика зависимости ускорения
_S_ и эффективности _E_ от количества процессов _p_,
где p = 1,2,3,…,8-12 для количества точек равного 2000, 10 000, 50 000 _(см. примечание 2)_.
Сделать выводы.
1) Подумать, каким образом следует организовать пересылку сообщений между процессами
посредством блокирующих функций приема/передачи,
чтобы суммарное время передачи в конце каждого шага по времени было _О(1)_ (а не _О(p)_).
Постараться реализовать оба варианта.

### HDFS

#### AA (0.1 балла)
Запишите в свою домашнюю директорию в HDFS файл `README`, содержащий:
* в первой строке - ваш e-mail
* во второй строке - имя и фамилию
Не путайте строки и имя с фамилией. Задача проверяется по наличию файла.

#### BB (0.1 балла)
Создайте текстовый файл размером не больше 5 MB. Желательно именно с текстом, а не с html, xml и прочей разметкой. Можно откуда-то скопировать, можно скачать. 
> Хорошо подходит для этого lib.ru, вот так можно получить всего "Евгения Онегина": 
> 	
> 	$ wget "http://lib.ru/LITRA/PUSHKIN/p4.txt"
> только возможно придется поправить кодировку одним из способов:
> 
> 	$ cat p4.txt | iconv -f windows-1251 -t utf8 p4_utf.txt
> 	$ cat p4.txt | iconv -f koi8-r -t utf8 p4_utf.txt
> Главное, чтобы в итоге команда:
> 
> 	$ more <file>
> выдавала читаемый текст.

Создайте в домашней директории в HDFS директорию text/full и запишите туда полученный файл. Выделите 50 первых строк файла в отдельный файл (удобно с помощью команды head), запишите в директорию text/sample. Задача проверяется по наличию директорий и файлов. *Не качайте все одно и то же!*

#### 01 (0.1 балла)
На вход скрипту подается имя файла, на выходе нужно получить имя сервера или IP-адрес, с которого будет читаться первый блок данных (реплик может быть несколько, засчитываться будет любой из них). Пример:

	$ ./run.sh /data/access_logs/big_log/access.log.2015-12-10
	mipt-node01.atp-fivt.org

#### 02 (0.1 балла)
На вход скрипту подается имя файла, на выходе нужно получить первые 10 байт этого файла (hadoop fs и hdfs dfs использовать нельзя)

	$ ./run.sh /data/access_logs/big_log/access.log.2015-12-10
	41.190.60.

#### 03 (0.1 балла)
На вход скрипту подается полный путь до файла в HDFS, на выходе нужно получить размер файла в блоках (см. hdfs fsck -h).

	$ ./run.sh /data/access_logs/big_log/access.log.2015-12-10	
	8

#### 04 (0.2 балла)

На вход скрипту подается идентификатор блока, на выходе нужно получить имя сервера (если их несколько, то выбрать любой), где хранится данный блок и физический путь в локальной файловой системе до этого блока данных. (О том, как зайти на ноды кластера, написано в материалам [cеминара по HDFS](/distribute/practice/01-hdfs.md))

	$ ./run.sh blk_1075127191
	bds03.vdi.mipt.ru:/dfs/dn/current/BP-76251478-10.55.163.141-1427134131440/current/finalized/subdir21/subdir35/blk_1075127191

#### 05 (0.3 балла)
Большие файлы на кластере делятся на блоки определенного размера. Нужно выяснить какой дополнительный объем используется в HDFS для хранения данных при использовании одной реплики (т.е. без дублирования данных). Для проведения экспериментов и создания файлов разного размера предлагается использовать утилиту dd (см. [пример использования](http://unix.stackexchange.com/questions/101332/generate-file-of-a-certain-size)) + решение задачи 4. На вход программы подается размер файла в байтах, на выходе программы должно быть одно число, равное числу байт для физического хранения этого файла (без учета хранения файлов с расширением .meta). Из полученного результата вычесть объём исходного файла (в байтах).

	$ ./run.sh <int>
	123
### HADOOP(MAP_REDUCE)

#### 1.(110) 
Список идентификаторов перемешайте в случайном порядке. Далее в каждой строке запишите через запятую случайное число идентификаторов - от 1 до 5.
Вариант способа перемешивания записей: дописать к каждой случайное число, отсортировать по нему весь список, потом число отбросить.
* Входные данные: список идентификаторов.
* Формат вывода: id1,id2,...
* Вывод на печать: первые 50 строк.

P.S. Задача из реальной жизни. Нужно было проверить сервер новой БД на нагрузку (“обстрелять”). Для этого нужно было сгенерировать запросы к ней (т.н.”патроны”), которые бы походили на реальные. Был известен список ключей, которые в этой базе могут быть, также было известно, что в одном запросе таких ключей до пяти штук.

Пример вывода:
```
1cf54b530128257d72,4cdf3efa01036a9a48,8c3e7fb30261aaf9cf
4cfe6230016553c3ed,76e1b8690176f801bb,e7409c39013c9db7b4,a5f1519c02b22550e6
83a119ef02346d0879
```
 
#### 5.(114)
 То же, что задача 4, только считаем также, сколько раз встретилось исходное слово. На выходе выписываем перестановку, ее популярность и первые 5 самых популярных исходных слов, для примера из задачи 2: abc 100 abc:50;bca:50
Сортировка та же, что в задаче 2. Исходные данные очищайте от знаков пунктуации, фильтруйте слова короче 3х символов.
* Входные данные: википедия.
* Формат вывода: перестановка <tab> сколько раз встретилась <tab> слово1:популярность1;…
* Вывод на печать: топ10 перестановок

Пример вывода:
```
asw	91833	was:90214;saw:1604;asw:11;aws:2;wsa:1;
for	89924	for:89912;fro:6;rof:4;rfo:1;ofr:1;
ahtt	81085	that:81085;
```

### HIVE


#### Задача 1 (411).
Создайте внешние (EXTERNAL) таблицы по исходным данным. В результате будет 4 таблицы: логи пользователей, данные ip адресов, данные пользователей, subnets. Из таблицы логов перенесите данные в другую таблицы, партицированную по датам – одна партиция на каждый день. На партиционированных таблицах и нужно будет выполнять запросы в следующих задачах.

Требуется, чтобы сериализация и десериализация данных осуществлялась с использованием регулярных выражений (см. `org.apache.hadoop.hive.contrib.serde2.RegexSerDe`, `org.apache.hadoop.hive.serde2.RegexSerDe`).

Проверить правильность создания таблиц с помощью простейших запросов (`SELECT * FROM <table> LIMIT 10`).

Дополнительные требования:

1.  Название вашей базы данных должно совпадать  **с логином на GitLab.atp-fivt.org**  (например,  **ivchenkoon**).
2.  Не добавляйте код создания базы в GitLab-репозиторий т.к. базы для вас уже созданы и система тестирования не имеет прав на перезаписть этих баз.
    
3.  Таблицы должны называться так:
    
4.  1.  Logs - партиционированная таблица с логами.
        
    2.  Users - таблица с информацией о пользователях.
        
    3.  IPRegions - таблица с IP и регионами.
        
    4.  Subnets - таблица с подсетями для 4-й и 6-й задач

#### Задача 2.1. (421).
Напишите запрос, выбирающий количество посещений для каждого дня. Полученные результаты отсортируйте по убыванию количества.

#### Задача 3.1. (441). 
Напишите запрос, выбирающий количество посещений от мужчин и от женщин по регионам.

#### Задача 4.1 (461).
 Создать UDF «перевертыш». Функция принимает строку и возвращает данную строку записанную в обратном порядке. Подключите данную функцию к hive и выполните выборку «перевертышей» для ip адресов. Вывести TOP-10 записей.

#### Задача 5.1. (471).
Представьте ситуацию, что все новостные сайты переехали в домен .com. Вас попросили обновить базу логов, чтоб логи пользователей указывали не на старые домены, а на новые. Например, новостная ссылка http://news.rambler.ru/8744806 теперь должна выглядеть в ваших запросах как http://news.rambler.com/8744806. Используйте стриминг в hive-sql запросе. (Рекомендуется обратить внимание на команды awk и sed). Выведите TOP-10 записей логов без сортировки.

### SPARK
#### 1. [0,5 балла]

Найдите все пары двух последовательных слов (биграмм), где первое слово «narodnaya». Для каждой пары подсчитайте количество вхождений в тексте статей Википедии. Выведите все пары с их частотой вхождений в лексикографическом порядке. Формат вывода - word_pair <tab> count.
